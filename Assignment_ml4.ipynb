{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4692464-cb7c-440c-bc25-a38f24cf604f",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc0a14-25f6-4424-a5e0-639043bb572b",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range. It transforms the data so that it falls within a specified interval, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "x_scaled=(x-x_min)/(x_max-x_min)\n",
    "\n",
    "where X is the original value, X_min is the minimum value in the feature, and X_max is the maximum value in the feature.\n",
    "\n",
    "Min-Max scaling is particularly useful when features have different scales, and you want to bring them to a common range. It helps prevent features with larger values from dominating the learning process when working with algorithms that are sensitive to the scale of the input features, such as neural networks or distance-based algorithms.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "Let's say we have a dataset with a feature representing house prices, ranging from 100000 to 1,000,000, and another feature representing the size of the houses in square feet, ranging from 500 to 3000. We want to normalize these features using Min-Max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb356da-ac00-452f-96c4-6bdd1ccf9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "House_Price= [100000, 500000, 1000000]\n",
    "House_Size=[500, 1500, 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d80d681-751f-4e05-b536-a87be27649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_House_Price: [(100000 - 100000) / (1000000 - 100000), (500000 - 100000) / (1000000 - 100000), (1000000 - 100000) / (1000000 - 100000)]\n",
    "Normalized_House_Size: [(500 - 500) / (3000 - 500), (1500 - 500) / (3000 - 500), (3000 - 500) / (3000 - 500)]\n",
    "\n",
    "                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f631518a-b862-4c7a-a1de-f5f6d815922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_House_Price: [0.0, 0.44444444, 1.0]\n",
    "Normalized_House_Size: [0.0, 0.4, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99aaac-02ef-4d88-a540-7e1152c5a18f",
   "metadata": {},
   "source": [
    "After Min-Max scaling, both features are now within the range of 0 to 1, making them comparable and suitable for use in machine learning models.\n",
    "\n",
    "It's important to note that Min-Max scaling should be applied separately to each feature and that the minimum and maximum values used for scaling should be calculated from the training data and then applied consistently to the test or unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded3281-64f9-42af-928b-72dfab995236",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6640b4e-96af-40e6-8d81-5613dc2610d4",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or feature scaling, rescales the features of a dataset to have a unit norm. It transforms each feature vector to a vector with a magnitude of 1 while preserving the direction of the original vector.\n",
    "\n",
    "Unlike Min-Max scaling, which rescales the data to a specific range (e.g., 0 to 1), Unit Vector scaling focuses on the relative magnitudes of the feature vectors rather than their absolute values. This technique is particularly useful when the magnitude of the features is important, but the scale or range of values is not crucial.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "x_scaled=x/||x||\n",
    "\n",
    "where X is the original feature vector, X_scaled is the normalized feature vector, and ||X|| represents the Euclidean norm or magnitude of the original feature vector.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two features: age (ranging from 20 to 60 years) and income (ranging from 20000 to 100,000). We want to apply Unit Vector scaling to these features.\n",
    "\n",
    "Original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6941c451-92b8-4226-ae61-ba596bd90839",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age=[20, 30, 40, 50, 60]\n",
    "Income= [20000, 40000, 60000, 80000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc28b02-4a21-4fc3-9887-6a9af9b0a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Age: [0.21081851067789198, 0.31622776601683794, 0.42163702135578396, 0.5270462766947299, 0.6324555320336759]\n",
      "Normalized Income: [0.13483997249264842, 0.26967994498529685, 0.40451991747794525, 0.5393598899705937, 0.6741998624632421]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "age_values = [20, 30, 40, 50, 60]\n",
    "income_values = [20000, 40000, 60000, 80000, 100000]\n",
    "\n",
    "normalized_age = [value / sqrt(sum(x**2 for x in age_values)) for value in age_values]\n",
    "normalized_income = [value / sqrt(sum(x**2 for x in income_values)) for value in income_values]\n",
    "\n",
    "print(\"Normalized Age:\", normalized_age)\n",
    "print(\"Normalized Income:\", normalized_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e48f9-cb2a-4656-a2f8-aa382f6feb93",
   "metadata": {},
   "source": [
    "After applying Unit Vector scaling, both feature vectors now have a magnitude of 1, reflecting their relative directions and magnitudes within the dataset.\n",
    "\n",
    "It's important to note that Unit Vector scaling should be applied separately to each feature vector and that the scaling parameters should be determined based on the training data and then consistently applied to the test or unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78444a6-9be7-40a7-8833-dd1b2e91e3d2",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a46fe-8eb6-4669-880e-89140a329ade",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a statistical technique used for dimensionality reduction. It aims to transform a dataset with a high number of variables into a smaller set of uncorrelated variables called principal components. These principal components capture the most important information and variability present in the original data.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works:\n",
    "\n",
    "Standardize the data: If the variables in the dataset have different scales or units, it is necessary to standardize them to have zero mean and unit variance. This step ensures that variables with larger scales do not dominate the PCA process.\n",
    "\n",
    "Calculate the covariance matrix: The covariance matrix is computed to measure the relationships between different variables in the dataset. It represents the degree of linear association between variables.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are derived from the covariance matrix. Eigenvectors represent the directions or axes in the original feature space, while eigenvalues indicate the importance or variance explained by each eigenvector.\n",
    "\n",
    "Select the principal components: The eigenvectors are ranked based on their corresponding eigenvalues in descending order. The top eigenvectors with the highest eigenvalues are selected as the principal components. These components capture most of the variance present in the original data.\n",
    "\n",
    "Project the data onto the new feature space: The selected principal components form a new feature space. The original data is projected onto this reduced-dimensional space, which results in a transformed dataset with a lower number of variables.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction to address problems such as the curse of dimensionality, multicollinearity, and visualization of high-dimensional data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Consider a dataset with four numerical variables: age, income, education, and work experience. We want to reduce the dimensionality of the dataset using PCA.\n",
    "\n",
    "Original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16bc0e2-11b6-4e9a-b2d4-fdae15dfcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age= [25, 30, 35, 40, 45]\n",
    "Income= [50000, 60000, 70000, 80000, 90000]\n",
    "Education= [12, 14, 16, 18, 20]\n",
    "Experience= [3, 6, 9, 12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26db15a4-21f0-4fc6-b9a0-0a86fd60231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal_Component_1: [0.61, 0.27, -0.11, -0.49, -0.28]\n",
    "Principal_Component_2: [-0.12, -0.44, -0.72, 0.45, 0.64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4c279-2c3f-4b64-a478-71290942676f",
   "metadata": {},
   "source": [
    "In this example, PCA reduced the dimensionality from four variables to two principal components. The transformed dataset now contains only two variables, capturing the most important information and variability present in the original data.\n",
    "\n",
    "It's important to note that PCA is an unsupervised technique and does not take into account the class labels or target variable. It focuses solely on capturing the variability in the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769f92b-49a6-41f2-90cf-17f6c8c33fa0",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b52eb-7b9c-49ef-be2f-6f022cff6bf9",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It is commonly used for feature extraction in machine learning and data analysis. Feature extraction refers to the process of transforming the original set of features into a reduced set of representative features that capture most of the relevant information in the data.\n",
    "\n",
    "PCA works by identifying the directions (principal components) in the feature space along which the data exhibits the most significant variation. These principal components are orthogonal to each other and are ranked in order of their importance, with the first component capturing the highest amount of variation, the second component capturing the second highest amount, and so on.\n",
    "\n",
    "PCA can be used for feature extraction by selecting a subset of the top-ranked principal components as the new set of features. These selected components, often referred to as \"principal features,\" can be used for further analysis or as inputs for machine learning algorithms.\n",
    "\n",
    "Here's an example to illustrate the concept:\n",
    "\n",
    "Suppose you have a dataset containing several numerical features, such as age, income, education level, and expenditure. You want to perform feature extraction to reduce the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "Standardize the data: Before applying PCA, it is recommended to standardize the features to have zero mean and unit variance. This step ensures that features with larger scales do not dominate the PCA process.\n",
    "\n",
    "Apply PCA: Calculate the principal components of the standardized dataset. Each principal component is a linear combination of the original features.\n",
    "\n",
    "Determine the explained variance: The PCA process also provides information about the amount of variance explained by each principal component. This information helps in understanding the significance of each component.\n",
    "\n",
    "Select the desired number of components: Depending on the desired level of dimensionality reduction, you can select a specific number of principal components that capture a significant amount of variance in the data. For example, you might choose to retain the top three principal components.\n",
    "\n",
    "Extract the principal features: Transform the original dataset using the selected principal components. This transformation results in a new dataset where each instance is represented by the extracted principal features.\n",
    "\n",
    "The resulting dataset with the extracted principal features can be used for further analysis or as input to machine learning algorithms. By reducing the dimensionality, PCA helps in eliminating irrelevant or redundant information and focuses on the most important aspects of the data.\n",
    "\n",
    "Note that while PCA can be used for feature extraction, it does not necessarily provide interpretable features in terms of the original feature space. The principal components are linear combinations of the original features and may not have a straightforward interpretation in the context of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f9f81-9dce-4c5a-995f-1af01feacbfa",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d7470-66be-4f34-a6d5-c9810b53f10d",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used as a preprocessing step to normalize the numerical features such as price, rating, and delivery time. Min-Max scaling rescales the values of a feature to a fixed range, typically between 0 and 1, based on the minimum and maximum values observed in the dataset. This normalization ensures that all features contribute equally to the analysis and prevents features with larger values from dominating the recommendation process.\n",
    "\n",
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "Identify the numerical features: In the given dataset, you mentioned that there are features such as price, rating, and delivery time. These features need to be preprocessed using Min-Max scaling.\n",
    "\n",
    "Compute the minimum and maximum values: Calculate the minimum and maximum values for each of the numerical features (price, rating, delivery time) in the dataset. This step helps in determining the range for rescaling the features.\n",
    "\n",
    "Apply Min-Max scaling: For each numerical feature, use the following formula to rescale the values to the range [0, 1]:\n",
    "\n",
    "rescaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula subtracts the minimum value from each data point and then divides it by the range (difference between the maximum and minimum values).\n",
    "\n",
    "Repeat this step for all numerical features, applying Min-Max scaling to each one.\n",
    "\n",
    "Normalized feature values: After applying Min-Max scaling, the numerical features will have values ranging from 0 to 1. This normalization ensures that all features are on a similar scale and prevents any particular feature from dominating the recommendation process due to its larger values.\n",
    "\n",
    "Use the preprocessed data for recommendation: The preprocessed dataset with Min-Max scaled features can now be used as input for the recommendation system. The normalized features will help in making fair comparisons between different items (restaurants or dishes) based on their price, rating, and delivery time.\n",
    "\n",
    "By applying Min-Max scaling, the recommendation system can effectively consider and weigh the importance of each feature without any bias towards features with larger values. This preprocessing step enhances the accuracy and reliability of the recommendation system by ensuring that all features contribute equally to the final recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfb790-92d1-4a31-8749-7ab4662c22de",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ceffb-bc2a-476d-bf45-f1d49206ee80",
   "metadata": {},
   "source": [
    "In the context of building a model to predict stock prices, Principal Component Analysis (PCA) can be used as a technique to reduce the dimensionality of the dataset. By reducing the number of features, PCA can help in mitigating the curse of dimensionality, improving computational efficiency, and potentially capturing the most significant information from the original dataset.\n",
    "\n",
    "Here's an explanation of how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Identify the features: In the given dataset, you mentioned that there are many features, including company financial data and market trends. These features contribute to the dimensionality of the dataset and may contain redundant or less informative information.\n",
    "\n",
    "Preprocess the data: Before applying PCA, it is generally recommended to preprocess the data by standardizing or normalizing the features. This step ensures that features with different scales do not dominate the PCA process.\n",
    "\n",
    "Apply PCA: Perform PCA on the preprocessed dataset. PCA calculates the principal components, which are linear combinations of the original features. These principal components capture the directions of maximum variance in the dataset.\n",
    "\n",
    "Determine the explained variance: After performing PCA, you will obtain the principal components along with the associated eigenvalues. The eigenvalues represent the amount of variance explained by each principal component. By examining the eigenvalues, you can determine the importance of each component in capturing the variance in the dataset.\n",
    "\n",
    "Select the desired number of components: To reduce the dimensionality of the dataset, you can select a subset of the top-ranked principal components that explain a significant portion of the total variance. The decision of how many components to retain depends on the desired level of dimensionality reduction and the trade-off between simplicity and preserving information.\n",
    "\n",
    "Transform the data: Transform the original dataset using the selected principal components. This transformation results in a new dataset with reduced dimensionality, where each instance is represented by the projected values onto the selected principal components.\n",
    "\n",
    "Use the reduced dataset for modeling: The transformed dataset with reduced dimensionality can be used as input for building the predictive model to forecast stock prices. By reducing the dimensionality, PCA helps in eliminating less significant features and focuses on the most important components that capture the maximum variation in the data.\n",
    "\n",
    "It's worth noting that while PCA can effectively reduce dimensionality, the resulting principal components may not have a direct interpretation in terms of the original features. However, they capture the most significant patterns and variations present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9475-4f08-4905-8463-83158b11375a",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d50101-7664-443f-a4d7-31c06b64346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e50845a-fddb-4b7d-9878-074591285ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cdc517-cd57-46b4-b3fb-479cd81132e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([1, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c50dd6-45f2-4104-b2cc-e289325d0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea025a7e-f599-4375-8f50-0b928ef4484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled=scaler.fit_transform(data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3534daf4-3b10-400a-9827-8b7c1b0bd3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(data_scaled.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e8fe6-220d-470f-b14b-f921afcf48ee",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872f5189-f352-441b-b8d4-e12805615294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Blood Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.143694</td>\n",
       "      <td>57.590303</td>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>101.510687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179.973454</td>\n",
       "      <td>66.870532</td>\n",
       "      <td>54</td>\n",
       "      <td>Male</td>\n",
       "      <td>125.397926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.829785</td>\n",
       "      <td>61.510532</td>\n",
       "      <td>60</td>\n",
       "      <td>Male</td>\n",
       "      <td>128.123140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154.937053</td>\n",
       "      <td>93.779526</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>126.032283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164.213997</td>\n",
       "      <td>76.575006</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>124.730791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Height     Weight  Age Gender  Blood Pressure\n",
       "0  159.143694  57.590303   26   Male      101.510687\n",
       "1  179.973454  66.870532   54   Male      125.397926\n",
       "2  172.829785  61.510532   60   Male      128.123140\n",
       "3  154.937053  93.779526   20   Male      126.032283\n",
       "4  164.213997  76.575006   22   Male      124.730791"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "height=np.random.normal(loc=170,scale=10,size=10000)\n",
    "weight=np.random.normal(loc=70,scale=10,size=10000)\n",
    "age=np.random.randint(18,65,size=10000)\n",
    "gender=np.random.choice(['Male','Female'],size=10000)\n",
    "blood_pressure=np.random.normal(loc=120,scale=10,size=10000)\n",
    "\n",
    "data=pd.DataFrame({'Height':height,\n",
    "                  'Weight':weight,\n",
    "                  'Age':age,\n",
    "                  'Gender':gender,\n",
    "                  'Blood Pressure':blood_pressure})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89ec92d-5e76-460a-86d5-14d470ce01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios:\n",
      "Principal Component 1:0.4001\n",
      "Principal Component 2:0.2023\n",
      "Principal Component 3:0.2006\n",
      "Principal Component 4:0.1971\n",
      "Principal Component 5:0.0000\n",
      "\n",
      "Cumulative Explained Variance:\n",
      "Principal Components 1:0.4001\n",
      "Principal Components 2:0.6024\n",
      "Principal Components 3:0.8029\n",
      "Principal Components 4:1.0000\n",
      "Principal Components 5:1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler , OneHotEncoder\n",
    "\n",
    "data_encoded=pd.get_dummies(data,columns=['Gender'])\n",
    "\n",
    "x=data_encoded.drop(columns=['Blood Pressure'])\n",
    "y=data_encoded['Blood Pressure']\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit_transform(x)\n",
    "\n",
    "pca=PCA()\n",
    "\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "explained_variance_ratio=pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_explained_variance=np.cumsum(explained_variance_ratio)\n",
    "\n",
    "\n",
    "print('Explained Variance Ratios:')\n",
    "for i , ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"Principal Component {i+1}:{ratio:.4f}\")\n",
    "    \n",
    "print(\"\\nCumulative Explained Variance:\")\n",
    "for i , variance in enumerate(cumulative_explained_variance):\n",
    "    print(f\"Principal Components {i+1}:{variance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e09a9-30ae-4b46-812b-79946b357ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
